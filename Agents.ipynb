{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf  \n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import tensorflow_addons.layers as layers\n",
    "\n",
    "\n",
    "from gym import spaces\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "\n",
    "import math\n",
    "\n",
    "try:\n",
    "    xrange = xrange\n",
    "except:\n",
    "    xrange = range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./online_shoppers_intention_PCA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Revenue = df.Revenue.apply(lambda x: 1 if x==True else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./osi_pca_final.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>PC11</th>\n",
       "      <th>PC12</th>\n",
       "      <th>PC13</th>\n",
       "      <th>PC14</th>\n",
       "      <th>PC15</th>\n",
       "      <th>PC16</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.104711</td>\n",
       "      <td>0.115576</td>\n",
       "      <td>-0.395859</td>\n",
       "      <td>-0.367811</td>\n",
       "      <td>0.792326</td>\n",
       "      <td>0.687524</td>\n",
       "      <td>-0.413624</td>\n",
       "      <td>0.404863</td>\n",
       "      <td>-0.295999</td>\n",
       "      <td>-0.032112</td>\n",
       "      <td>-0.020025</td>\n",
       "      <td>0.141937</td>\n",
       "      <td>0.128971</td>\n",
       "      <td>-0.100672</td>\n",
       "      <td>-0.406829</td>\n",
       "      <td>0.845749</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.142662</td>\n",
       "      <td>-0.271433</td>\n",
       "      <td>-0.129434</td>\n",
       "      <td>0.178537</td>\n",
       "      <td>-0.074198</td>\n",
       "      <td>-0.258909</td>\n",
       "      <td>0.315821</td>\n",
       "      <td>-0.120330</td>\n",
       "      <td>0.054817</td>\n",
       "      <td>0.010389</td>\n",
       "      <td>0.211168</td>\n",
       "      <td>0.171537</td>\n",
       "      <td>0.018455</td>\n",
       "      <td>-0.577434</td>\n",
       "      <td>0.774437</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.110375</td>\n",
       "      <td>0.146153</td>\n",
       "      <td>-0.392492</td>\n",
       "      <td>-0.241203</td>\n",
       "      <td>0.771216</td>\n",
       "      <td>0.936403</td>\n",
       "      <td>0.567604</td>\n",
       "      <td>0.373728</td>\n",
       "      <td>-0.208422</td>\n",
       "      <td>-0.012134</td>\n",
       "      <td>-0.036700</td>\n",
       "      <td>0.155926</td>\n",
       "      <td>0.128702</td>\n",
       "      <td>0.008547</td>\n",
       "      <td>-0.345934</td>\n",
       "      <td>0.866735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.034761</td>\n",
       "      <td>0.133210</td>\n",
       "      <td>-0.310407</td>\n",
       "      <td>-0.177025</td>\n",
       "      <td>0.365475</td>\n",
       "      <td>0.202037</td>\n",
       "      <td>-0.171265</td>\n",
       "      <td>0.328262</td>\n",
       "      <td>-0.037567</td>\n",
       "      <td>0.074858</td>\n",
       "      <td>-0.003444</td>\n",
       "      <td>0.192169</td>\n",
       "      <td>0.157149</td>\n",
       "      <td>0.051173</td>\n",
       "      <td>-0.515762</td>\n",
       "      <td>0.798940</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-0.123149</td>\n",
       "      <td>0.048001</td>\n",
       "      <td>0.711157</td>\n",
       "      <td>-0.095758</td>\n",
       "      <td>0.307547</td>\n",
       "      <td>-0.108562</td>\n",
       "      <td>-0.230177</td>\n",
       "      <td>0.309132</td>\n",
       "      <td>0.017062</td>\n",
       "      <td>0.116845</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>0.205523</td>\n",
       "      <td>0.176376</td>\n",
       "      <td>0.135875</td>\n",
       "      <td>-0.541553</td>\n",
       "      <td>0.803390</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PC1       PC2       PC3       PC4       PC5       PC6       PC7  \\\n",
       "0  0.104711  0.115576 -0.395859 -0.367811  0.792326  0.687524 -0.413624   \n",
       "1  0.000601  0.142662 -0.271433 -0.129434  0.178537 -0.074198 -0.258909   \n",
       "2  0.110375  0.146153 -0.392492 -0.241203  0.771216  0.936403  0.567604   \n",
       "3  0.034761  0.133210 -0.310407 -0.177025  0.365475  0.202037 -0.171265   \n",
       "4 -0.123149  0.048001  0.711157 -0.095758  0.307547 -0.108562 -0.230177   \n",
       "\n",
       "        PC8       PC9      PC10      PC11      PC12      PC13      PC14  \\\n",
       "0  0.404863 -0.295999 -0.032112 -0.020025  0.141937  0.128971 -0.100672   \n",
       "1  0.315821 -0.120330  0.054817  0.010389  0.211168  0.171537  0.018455   \n",
       "2  0.373728 -0.208422 -0.012134 -0.036700  0.155926  0.128702  0.008547   \n",
       "3  0.328262 -0.037567  0.074858 -0.003444  0.192169  0.157149  0.051173   \n",
       "4  0.309132  0.017062  0.116845  0.001683  0.205523  0.176376  0.135875   \n",
       "\n",
       "       PC15      PC16  Revenue  \n",
       "0 -0.406829  0.845749        0  \n",
       "1 -0.577434  0.774437        0  \n",
       "2 -0.345934  0.866735        0  \n",
       "3 -0.515762  0.798940        0  \n",
       "4 -0.541553  0.803390        0  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>PC3</th>\n",
       "      <th>PC4</th>\n",
       "      <th>PC5</th>\n",
       "      <th>PC6</th>\n",
       "      <th>PC7</th>\n",
       "      <th>PC8</th>\n",
       "      <th>PC9</th>\n",
       "      <th>PC10</th>\n",
       "      <th>PC11</th>\n",
       "      <th>PC12</th>\n",
       "      <th>PC13</th>\n",
       "      <th>PC14</th>\n",
       "      <th>PC15</th>\n",
       "      <th>PC16</th>\n",
       "      <th>Revenue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>1.233000e+04</td>\n",
       "      <td>12330.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>-2.833095e-16</td>\n",
       "      <td>1.196934e-16</td>\n",
       "      <td>3.015521e-17</td>\n",
       "      <td>4.003196e-16</td>\n",
       "      <td>-5.379134e-17</td>\n",
       "      <td>3.293752e-17</td>\n",
       "      <td>2.687316e-17</td>\n",
       "      <td>-1.413092e-16</td>\n",
       "      <td>-9.792113e-18</td>\n",
       "      <td>-6.435219e-18</td>\n",
       "      <td>6.553400e-18</td>\n",
       "      <td>1.355082e-17</td>\n",
       "      <td>2.225680e-17</td>\n",
       "      <td>2.050041e-17</td>\n",
       "      <td>3.956239e-18</td>\n",
       "      <td>-1.490990e-17</td>\n",
       "      <td>0.154745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>5.202533e-01</td>\n",
       "      <td>4.419048e-01</td>\n",
       "      <td>4.233530e-01</td>\n",
       "      <td>3.935412e-01</td>\n",
       "      <td>3.568858e-01</td>\n",
       "      <td>3.178981e-01</td>\n",
       "      <td>2.988291e-01</td>\n",
       "      <td>2.536931e-01</td>\n",
       "      <td>2.134450e-01</td>\n",
       "      <td>2.006860e-01</td>\n",
       "      <td>1.883838e-01</td>\n",
       "      <td>1.736535e-01</td>\n",
       "      <td>1.590260e-01</td>\n",
       "      <td>1.487774e-01</td>\n",
       "      <td>1.296181e-01</td>\n",
       "      <td>1.230221e-01</td>\n",
       "      <td>0.361676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-8.402723e-01</td>\n",
       "      <td>-6.953180e-01</td>\n",
       "      <td>-5.190144e-01</td>\n",
       "      <td>-9.752206e-01</td>\n",
       "      <td>-9.327325e-01</td>\n",
       "      <td>-5.354765e-01</td>\n",
       "      <td>-6.710074e-01</td>\n",
       "      <td>-3.505844e-01</td>\n",
       "      <td>-4.226392e-01</td>\n",
       "      <td>-7.747611e-01</td>\n",
       "      <td>-6.879743e-01</td>\n",
       "      <td>-3.367644e-01</td>\n",
       "      <td>-2.531806e-01</td>\n",
       "      <td>-4.726478e-01</td>\n",
       "      <td>-6.070274e-01</td>\n",
       "      <td>-2.247490e-01</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>-3.226690e-01</td>\n",
       "      <td>-3.840497e-01</td>\n",
       "      <td>-2.702411e-01</td>\n",
       "      <td>-1.226329e-01</td>\n",
       "      <td>-1.682379e-01</td>\n",
       "      <td>-2.129403e-01</td>\n",
       "      <td>-2.114201e-01</td>\n",
       "      <td>-1.608177e-01</td>\n",
       "      <td>-1.303764e-01</td>\n",
       "      <td>-5.633948e-02</td>\n",
       "      <td>-1.267446e-02</td>\n",
       "      <td>-3.509413e-02</td>\n",
       "      <td>-2.168313e-02</td>\n",
       "      <td>-9.596494e-02</td>\n",
       "      <td>-7.948566e-02</td>\n",
       "      <td>-5.882743e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>-1.004782e-01</td>\n",
       "      <td>-1.544297e-01</td>\n",
       "      <td>-1.591310e-01</td>\n",
       "      <td>-2.892698e-02</td>\n",
       "      <td>-3.229022e-02</td>\n",
       "      <td>-1.089591e-01</td>\n",
       "      <td>-7.771109e-02</td>\n",
       "      <td>-8.284529e-02</td>\n",
       "      <td>-6.712488e-02</td>\n",
       "      <td>-2.757171e-02</td>\n",
       "      <td>-2.722140e-03</td>\n",
       "      <td>8.079278e-04</td>\n",
       "      <td>-9.721898e-03</td>\n",
       "      <td>-8.366986e-03</td>\n",
       "      <td>-2.058079e-02</td>\n",
       "      <td>-2.165046e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>6.217818e-01</td>\n",
       "      <td>5.050762e-01</td>\n",
       "      <td>-3.112866e-02</td>\n",
       "      <td>7.820922e-02</td>\n",
       "      <td>1.540739e-01</td>\n",
       "      <td>1.370944e-01</td>\n",
       "      <td>1.575439e-01</td>\n",
       "      <td>-4.235357e-02</td>\n",
       "      <td>5.336724e-02</td>\n",
       "      <td>4.604629e-02</td>\n",
       "      <td>8.190347e-03</td>\n",
       "      <td>1.773221e-02</td>\n",
       "      <td>2.180568e-05</td>\n",
       "      <td>5.657598e-02</td>\n",
       "      <td>6.344650e-02</td>\n",
       "      <td>3.029083e-02</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>1.029452e+00</td>\n",
       "      <td>7.921294e-01</td>\n",
       "      <td>9.562168e-01</td>\n",
       "      <td>1.259046e+00</td>\n",
       "      <td>1.389758e+00</td>\n",
       "      <td>1.792742e+00</td>\n",
       "      <td>8.112542e-01</td>\n",
       "      <td>8.474756e-01</td>\n",
       "      <td>1.116463e+00</td>\n",
       "      <td>7.857294e-01</td>\n",
       "      <td>8.005585e-01</td>\n",
       "      <td>1.169694e+00</td>\n",
       "      <td>9.880411e-01</td>\n",
       "      <td>1.274954e+00</td>\n",
       "      <td>1.025581e+00</td>\n",
       "      <td>9.993194e-01</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                PC1           PC2           PC3           PC4           PC5  \\\n",
       "count  1.233000e+04  1.233000e+04  1.233000e+04  1.233000e+04  1.233000e+04   \n",
       "mean  -2.833095e-16  1.196934e-16  3.015521e-17  4.003196e-16 -5.379134e-17   \n",
       "std    5.202533e-01  4.419048e-01  4.233530e-01  3.935412e-01  3.568858e-01   \n",
       "min   -8.402723e-01 -6.953180e-01 -5.190144e-01 -9.752206e-01 -9.327325e-01   \n",
       "25%   -3.226690e-01 -3.840497e-01 -2.702411e-01 -1.226329e-01 -1.682379e-01   \n",
       "50%   -1.004782e-01 -1.544297e-01 -1.591310e-01 -2.892698e-02 -3.229022e-02   \n",
       "75%    6.217818e-01  5.050762e-01 -3.112866e-02  7.820922e-02  1.540739e-01   \n",
       "max    1.029452e+00  7.921294e-01  9.562168e-01  1.259046e+00  1.389758e+00   \n",
       "\n",
       "                PC6           PC7           PC8           PC9          PC10  \\\n",
       "count  1.233000e+04  1.233000e+04  1.233000e+04  1.233000e+04  1.233000e+04   \n",
       "mean   3.293752e-17  2.687316e-17 -1.413092e-16 -9.792113e-18 -6.435219e-18   \n",
       "std    3.178981e-01  2.988291e-01  2.536931e-01  2.134450e-01  2.006860e-01   \n",
       "min   -5.354765e-01 -6.710074e-01 -3.505844e-01 -4.226392e-01 -7.747611e-01   \n",
       "25%   -2.129403e-01 -2.114201e-01 -1.608177e-01 -1.303764e-01 -5.633948e-02   \n",
       "50%   -1.089591e-01 -7.771109e-02 -8.284529e-02 -6.712488e-02 -2.757171e-02   \n",
       "75%    1.370944e-01  1.575439e-01 -4.235357e-02  5.336724e-02  4.604629e-02   \n",
       "max    1.792742e+00  8.112542e-01  8.474756e-01  1.116463e+00  7.857294e-01   \n",
       "\n",
       "               PC11          PC12          PC13          PC14          PC15  \\\n",
       "count  1.233000e+04  1.233000e+04  1.233000e+04  1.233000e+04  1.233000e+04   \n",
       "mean   6.553400e-18  1.355082e-17  2.225680e-17  2.050041e-17  3.956239e-18   \n",
       "std    1.883838e-01  1.736535e-01  1.590260e-01  1.487774e-01  1.296181e-01   \n",
       "min   -6.879743e-01 -3.367644e-01 -2.531806e-01 -4.726478e-01 -6.070274e-01   \n",
       "25%   -1.267446e-02 -3.509413e-02 -2.168313e-02 -9.596494e-02 -7.948566e-02   \n",
       "50%   -2.722140e-03  8.079278e-04 -9.721898e-03 -8.366986e-03 -2.058079e-02   \n",
       "75%    8.190347e-03  1.773221e-02  2.180568e-05  5.657598e-02  6.344650e-02   \n",
       "max    8.005585e-01  1.169694e+00  9.880411e-01  1.274954e+00  1.025581e+00   \n",
       "\n",
       "               PC16       Revenue  \n",
       "count  1.233000e+04  12330.000000  \n",
       "mean  -1.490990e-17      0.154745  \n",
       "std    1.230221e-01      0.361676  \n",
       "min   -2.247490e-01      0.000000  \n",
       "25%   -5.882743e-02      0.000000  \n",
       "50%   -2.165046e-02      0.000000  \n",
       "75%    3.029083e-02      0.000000  \n",
       "max    9.993194e-01      1.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lamb_da = 1908/10422\n",
    "class CartPoleEnv(gym.Env):\n",
    " \n",
    "    def __init__(self):\n",
    "\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space = spaces.Box(-1, 10, shape=(16,), dtype=np.float32)\n",
    "\n",
    "        self.viewer = None\n",
    "        self.state = None\n",
    "\n",
    "        self.steps_beyond_done = None\n",
    "        self.episode = None\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        episode_df= df.sample(frac=0.8)\n",
    "        self.episode = episode_df.drop(columns=['Revenue']).values.tolist()\n",
    "        self.true_labels = episode_df['Revenue'].values.tolist()\n",
    "        \n",
    "        try:\n",
    "            self.state = self.episode.pop()\n",
    "        except:\n",
    "            self.state = None\n",
    "#         self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)\n",
    "#     .reshape(-1,3)\n",
    "    \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "        state = self.episode.pop()\n",
    "        \n",
    "#     try and get the true label from list self.rewards\n",
    "#     if none then done\n",
    "        done = False\n",
    "        true_label = self.true_labels.pop()\n",
    "    \n",
    "        if true_label is not None:\n",
    "            if (true_label == 1) & (action == true_label):\n",
    "                reward = 1\n",
    "            elif (true_label == 0) & (action == true_label):\n",
    "                reward = lamb_da\n",
    "            elif (true_label == 0) & (action != true_label):\n",
    "                reward = -lamb_da\n",
    "            else:\n",
    "                reward = -1\n",
    "                done = True\n",
    "        else:\n",
    "            reward = 0\n",
    "            done = True\n",
    "        \n",
    "        return np.array(state), reward, done, {}\n",
    "\n",
    "#     def render(self, mode='human'):\n",
    "        \n",
    "#         try:\n",
    "#             self.state = self.episode.pop()\n",
    "#         except:\n",
    "#             self.state = None\n",
    "            \n",
    "#         if self.state is None: return None\n",
    "\n",
    "#         return np.array(self.state) #.reshape(-1,3)\n",
    "    \n",
    "env = CartPoleEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "H = 32 # number of hidden layer neurons\n",
    "batch_size = 7 # every how many episodes to do a param update?\n",
    "learning_rate = 0.01 # feel free to play with this to train faster or more stably.\n",
    "gamma = 0.95 # discount factor for reward\n",
    "learning_decay = 50 #Learning rate decay\n",
    "D = 16 # input dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 1\n",
    "# Batch layer\n",
    "# Pooling layer\n",
    "# drop out layer\n",
    "# layer 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "\n",
    "#This defines the network as it goes from taking an observation of the environment to \n",
    "#giving a probability of chosing to the action of moving left or right.\n",
    "observations = tf.compat.v1.placeholder(tf.float32, [None,D] , name=\"input_x\")\n",
    "W1 = tf.compat.v1.get_variable(\"W1\", shape=[D, H],\n",
    "           initializer=tf.initializers.GlorotUniform())\n",
    "layer1 = tf.nn.tanh(tf.matmul(observations,W1))\n",
    "\n",
    "W11 = tf.compat.v1.get_variable(\"W11\", shape=[H, 8],\n",
    "           initializer=tf.initializers.GlorotUniform())\n",
    "\n",
    "\n",
    "layer11 = tf.nn.relu(tf.matmul(layer1, W11))\n",
    "\n",
    "\n",
    "W2 = tf.compat.v1.get_variable(\"W2\", shape=[8, 1],\n",
    "           initializer=tf.initializers.GlorotUniform())\n",
    "score = tf.matmul(layer11,W2)\n",
    "probability = tf.nn.sigmoid(score)\n",
    "\n",
    "#From here we define the parts of the network needed for learning a good policy.\n",
    "tvars = tf.compat.v1.trainable_variables()\n",
    "input_y = tf.compat.v1.placeholder(tf.float32,[None,1], name=\"input_y\")\n",
    "advantages = tf.compat.v1.placeholder(tf.float32,name=\"reward_signal\")\n",
    "\n",
    "# The loss function. This sends the weights in the direction of making actions \n",
    "# that gave good advantage (reward over time) more likely, and actions that didn't less likely.\n",
    "loglik = tf.compat.v1.log(input_y*(input_y - probability) + (1 - input_y)*(input_y + probability))\n",
    "loss = -tf.reduce_mean(loglik * advantages) \n",
    "newGrads = tf.gradients(loss,tvars)\n",
    "\n",
    "# Once we have collected a series of gradients from multiple episodes, we apply them.\n",
    "# We don't just apply gradeients after every episode in order to account for noise in the reward signal.\n",
    "adam = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate) # Our optimizer\n",
    "W1Grad = tf.compat.v1.placeholder(tf.float32,name=\"batch_grad1\")\n",
    "W11Grad = tf.compat.v1.placeholder(tf.float32,name=\"batch_grad11\")# Placeholders to send the final gradients through when we update.\n",
    "W2Grad = tf.compat.v1.placeholder(tf.float32,name=\"batch_grad2\")\n",
    "batchGrad = [W1Grad,W11Grad,W2Grad]\n",
    "updateGrads = adam.apply_gradients(zip(batchGrad,tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'W1:0' shape=(16, 32) dtype=float32_ref>,\n",
       " <tf.Variable 'W11:0' shape=(32, 8) dtype=float32_ref>,\n",
       " <tf.Variable 'W2:0' shape=(8, 1) dtype=float32_ref>]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10422\n",
       "1     1908\n",
       "Name: Revenue, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Revenue.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[]\n",
    "running_reward = None\n",
    "reward_sum = 0\n",
    "episode_number = 1\n",
    "total_episodes = 10000\n",
    "init = tf.compat.v1.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for episode -1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 0.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 0.000000.  Total average reward -1.000000.\n",
      "Average reward for episode -1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode -1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode -1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode -1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode -1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode -1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 0.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode -1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 2.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 0.000000.  Total average reward -1.000000.\n",
      "Average reward for episode -1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 0.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 0.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 1.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 0.000000.  Total average reward -1.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 3.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 1.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 2.000000.  Total average reward 0.000000.\n",
      "Average reward for episode 0.000000.  Total average reward 0.000000.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-35fc2d7d2442>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     88\u001b[0m                 \u001b[0mreward_sum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mepisode_number\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-a666bb5b91f8>\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mepisode_df\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrac\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepisode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepisode_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Revenue'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrue_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepisode_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Revenue'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Launch the graph\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    rendering = False\n",
    "    sess.run(init)\n",
    "    observation = env.reset() # Obtain an initial observation of the environment\n",
    "\n",
    "    # Reset the gradient placeholder. We will collect gradients in \n",
    "    # gradBuffer until we are ready to update our policy network. \n",
    "    gradBuffer = sess.run(tvars)\n",
    "    for ix,grad in enumerate(gradBuffer):\n",
    "        gradBuffer[ix] = grad * 0\n",
    "    \n",
    "    while episode_number <= total_episodes:\n",
    "        \n",
    "        # Rendering the environment slows things down, \n",
    "        # so let's only look at it once our agent is doing a good job.\n",
    "        #if reward_sum/batch_size > 100 or rendering == True : \n",
    "            #env.render()\n",
    "            #rendering = True\n",
    "            #print (reward_sum)\n",
    "        \n",
    "        # Make sure the observation is in a shape the network can handle.\n",
    "        x = np.reshape(observation,[1,D])\n",
    "        \n",
    "        # Run the policy network and get an action to take. \n",
    "        tfprob = sess.run(probability,feed_dict={observations: x})\n",
    "        action = 1 if np.random.uniform() < tfprob else 0\n",
    "        \n",
    "#         if (tfprob<0.01) & (tfprob < 0.5):\n",
    "#             action = np.random.choice(np.array([0,1]))\n",
    "#         else:\n",
    "#             action = int(tfprob)\n",
    "        \n",
    "        xs.append(x) # observation\n",
    "        y = 1 if action == 0 else 0 # a \"fake label\"\n",
    "        ys.append(y)\n",
    "\n",
    "        # step the environment and get new measurements\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        #print(reward)\n",
    "        reward_sum += reward\n",
    "\n",
    "        drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n",
    "        \n",
    "        \n",
    "        if done: \n",
    "            episode_number += 1\n",
    "            # stack together all inputs, hidden states, action gradients, and rewards for this episode\n",
    "            epx = np.vstack(xs)\n",
    "            epy = np.vstack(ys)\n",
    "            epr = np.vstack(drs)\n",
    "            tfp = tfps\n",
    "            xs,hs,dlogps,drs,ys,tfps = [],[],[],[],[],[] # reset array memory\n",
    "\n",
    "            # compute the discounted reward backwards through time\n",
    "            discounted_epr = discount_rewards(epr)\n",
    "            # size the rewards to be unit normal (helps control the gradient estimator variance)\n",
    "            #discounted_epr = discounted_epr - np.mean(discounted_epr)\n",
    "            #discounted_epr = discounted_epr//np.std(discounted_epr)\n",
    "            \n",
    "            # Get the gradient for this episode, and save it in the gradBuffer\n",
    "            tGrad = sess.run(newGrads,feed_dict={observations: epx, input_y: epy, advantages: discounted_epr})\n",
    "            for ix,grad in enumerate(tGrad):\n",
    "                gradBuffer[ix] += grad\n",
    "                \n",
    "            # If we have completed enough episodes, then update the policy network with our gradients.\n",
    "            if episode_number % batch_size == 0: \n",
    "                sess.run(updateGrads,feed_dict={W1Grad: gradBuffer[0],W11Grad:gradBuffer[1],W2Grad:gradBuffer[2]})\n",
    "                for ix,grad in enumerate(gradBuffer):\n",
    "                    gradBuffer[ix] = grad * 0\n",
    "                \n",
    "                # Give a summary of how well our network is doing for each batch of episodes.\n",
    "                running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n",
    "                #if running_reward > 70:\n",
    "                print('Average reward for episode %f.  Total average reward %f.' % (reward_sum//batch_size, running_reward//batch_size))\n",
    "                \n",
    "                if reward_sum > 2000: \n",
    "                    print(\"Task solved in\",episode_number,'episodes!')\n",
    "                    break\n",
    "                    \n",
    "                reward_sum = 0\n",
    "            \n",
    "            observation = env.reset()\n",
    "        \n",
    "            if episode_number % 2000 == 0:\n",
    "                \n",
    "                learning_rate -= 0.001 \n",
    "                print(learning_rate)\n",
    "            \n",
    "print(episode_number,'Episodes completed.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.737946999085467e-05"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.01 * np.exp(0.1*(10-60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mul_3:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.0"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "running_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reset_default_graph()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# #These lines establish the feed-forward part of the network used to choose actions\n",
    "# inputs1 = tf.placeholder(shape=[1,16],dtype=tf.float32)\n",
    "# W = tf.Variable(tf.random_uniform([16,4],0,0.01))\n",
    "# Qout = tf.matmul(inputs1,W)\n",
    "# predict = tf.argmax(Qout,1)\n",
    "\n",
    "# #Below we obtain the loss by taking the sum of squares difference between the target and prediction Q values.\n",
    "# nextQ = tf.placeholder(shape=[1,4],dtype=tf.float32)\n",
    "# loss = tf.reduce_sum(tf.square(nextQ - Qout))\n",
    "# trainer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "# updateModel = trainer.minimize(loss)\n",
    "\n",
    "# init = tf.global_variables_initializer()\n",
    "\n",
    "# # Set learning parameters\n",
    "# y = .99\n",
    "# e = 0.1\n",
    "# num_episodes = 2000\n",
    "# #create lists to contain total rewards and steps per episode\n",
    "# jList = []\n",
    "# rList = []\n",
    "# with tf.Session() as sess:\n",
    "#     sess.run(init)\n",
    "#     for i in range(num_episodes):\n",
    "#         #Reset environment and get first new observation\n",
    "#         s = env.reset()\n",
    "#         rAll = 0\n",
    "#         d = False\n",
    "#         j = 0\n",
    "#         #The Q-Network\n",
    "#         while j < 99:\n",
    "#             j+=1\n",
    "#             #Choose an action by greedily (with e chance of random action) from the Q-network\n",
    "#             a,allQ = sess.run([predict,Qout],feed_dict={inputs1:np.identity(16)[s:s+1]})\n",
    "#             if np.random.rand(1) < e:\n",
    "#                 a[0] = env.action_space.sample()\n",
    "#             #Get new state and reward from environment\n",
    "#             s1,r,d,_ = env.step(a[0])\n",
    "#             #Obtain the Q' values by feeding the new state through our network\n",
    "#             Q1 = sess.run(Qout,feed_dict={inputs1:np.identity(16)[s1:s1+1]})\n",
    "#             #Obtain maxQ' and set our target value for chosen action.\n",
    "#             maxQ1 = np.max(Q1)\n",
    "#             targetQ = allQ\n",
    "#             targetQ[0,a[0]] = r + y*maxQ1\n",
    "#             #Train our network using target and predicted Q values\n",
    "#             _,W1 = sess.run([updateModel,W],feed_dict={inputs1:np.identity(16)[s:s+1],nextQ:targetQ})\n",
    "#             rAll += r\n",
    "#             s = s1\n",
    "#             if d == True:\n",
    "#                 #Reduce chance of random action as we train the model.\n",
    "#                 e = 1./((i/50) + 10)\n",
    "#                 break\n",
    "#         jList.append(j)\n",
    "#         rList.append(rAll)\n",
    "# print(\"Percent of succesful episodes: \" + str(sum(rList)/num_episodes) + \"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
